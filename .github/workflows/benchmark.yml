name: "benchmark"

on:
  # Run using manual triggers from GitHub UI:
  # https://docs.github.com/en/actions/managing-workflow-runs/manually-running-a-workflow
  workflow_dispatch:
    inputs:
      dryRun:
        description: "Attempt a local run and report results here, without updating the bencher dashboard."
        type: "boolean"
        required: true
        default: true

  # Run on pushes to 'main' branch:
  push:
    branches:
      - "main"

# Queue up benchmark workflows for the same branch, so that results are reported in order:
concurrency:
  group: "${{ github.workflow }}-${{ github.ref_name }}"
  cancel-in-progress: false

jobs:
  benchmark:
    runs-on: "ubuntu-22.04" # _SLANG_DEV_CONTAINER_BASE_IMAGE_ (keep in sync)

    # Only run on the main repo (not forks), unless it is a dry run:
    if: "${{ github.repository == 'NomicFoundation/slang' || inputs.dryRun == true }}"

    steps:
      - name: "Checkout Repository"
        uses: "actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683"

      - name: "Restore Cache"
        uses: "./.github/actions/cache/restore"

      - name: "infra setup"
        run: "./scripts/bin/infra setup"

      - name: "infra perf benchmark"
        run: "./scripts/bin/infra perf benchmark ${{ inputs.dryRun == true && '--dry-run' || '' }}"
        env:
          BENCHER_API_TOKEN: "${{ secrets.BENCHER_API_TOKEN }}"

      - name: "Upload Benchmarking Data"
        uses: "actions/upload-artifact@65c4c4a1ddee5b72f698fdd19549f0f0fb45cf08"
        with:
          name: "benchmarking-data"
          path: "target/iai"
          if-no-files-found: "error"
